{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version was created for comparison of different types of algoritm, when the main version is for hsowing how classification works in full dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы сформированные ниже содержат отчеты по testing data для обрезанного датасета \n",
    "result900_of_random_forest.txt - Random Forest\n",
    "result900_of_GBC.txt - Gradient Boosting Classif\n",
    "DecisionTreeClassifier - дерево решений\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertPreTrainedModel, BertForPreTraining, BertForMaskedLM\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "import re\n",
    "#RUBERT_PATH = 'D:\\\\models\\\\rubert_cased_L-12_H-768_A-12_pt'\n",
    "#C:\\Users\\Lenovo\\Documents\\NLP\\BERT\\pytorch\\ru_conversational_cased_L-12_H-768_A-12_pt.tar\n",
    "RUBERT_PATH = 'C:\\\\Users\\\\Lenovo\\\\Documents\\\\NLP\\\\BERT\\\\pytorch\\\\ru_conversational_cased_L-12_H-768_A-12_pt.tar\\\\ru_conversational_cased_L-12_H-768_A-12_pt'\n",
    "modelpath = os.path.join(RUBERT_PATH,'pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(RUBERT_PATH, do_lower_case=False)\n",
    "config = BertConfig.from_json_file(os.path.join(RUBERT_PATH,'bert_config.json'))\n",
    "model = BertForPreTraining.from_pretrained(modelpath, config=config)\n",
    "model.eval()\n",
    "from torch import load\n",
    "di = load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с ограничениями вычислительной мощности, я сократила число строк до <900 (300 с каждой оценкой). И количество оценок(1 -низкий, 5 - средний, 10 - хороший). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homework4(text): \n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    segments_ids[0] = 0\n",
    "    #if (len(indexed_tokens)>1):\n",
    "    #    segments_ids[1] = 0\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    #if (len(indexed_tokens)>1):\n",
    "    #    segments_tensors = torch.tensor([segments_ids])\n",
    "    predictions = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    array_of_thensors = []\n",
    "    #ln  = 0\n",
    "    for i in range(len(tokenized_text)):\n",
    "        #array_of_thensors = array_of_thensors + predictions[0][0][i]\n",
    "        array_of_thensors.append((predictions[0][0][i].detach().numpy()))\n",
    "        #ln = ln +1 \n",
    "    mean = np.mean(array_of_thensors, axis = 0)\n",
    "    #all information is in file \"five_thousands_sorted_comments.csv\" - comments with grades\n",
    "    #\"df_with_thensors.csv\" - comment with tensor. Only 6 numbers from tensors, because when I tried to collect all \n",
    "    #length it contains more than 10 GB \n",
    "    #print(text)\n",
    "    #print(mean)\n",
    "    #print('######################')\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to implement this algorithm with preprocessing (deleting stopords and punctuations, but accuracy was decreased by 2-3%, so i decised to skip this step. If it will be needed, I can add here proof of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  grade\n",
      "0  думал, что будет лучше идея очень интересна - ...      7\n",
      "1  с творчеством Головачева я познакомился посред...     10\n",
      "2  то-то я и в большое неудовольствие прочитал \"А...      5\n",
      "3  как мне показалось местами сильно смахивает на...      6\n",
      "4  от первой части книги просто оторваться не мог...      9\n"
     ]
    }
   ],
   "source": [
    "df= pd.DataFrame()\n",
    "df = pd.read_csv('texts_train.txt', sep=\"\\t\", encoding='UTF-8',header=0)\n",
    "df['grade'] = pd.read_csv('scores_train.txt', sep=\"\\t\", encoding='UTF-8',header=0)\n",
    "df.columns = ['comment', 'grade']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem, which was described via whats up chat(according the limitization of computer power of the laptop). So I reduce the number of rows and for better classification choose 500 objects of every existing classes to have 5000 rows at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all unique grades:  [ 1  2  3  4  5  6  7  8  9 10]\n",
      "new unique grades:  [1, 5, 10]\n"
     ]
    }
   ],
   "source": [
    "unique_grades = np.sort(df['grade'].unique())\n",
    "print('all unique grades: ',unique_grades)\n",
    "unique_grades  = [1,5,10]\n",
    "print('new unique grades: ',unique_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди уникальных оценок выбрала только 1, 5, 10 низкий, средний, высокий эмоциональный окрас соответственно. Для вычисления точности заменила их на 1,2,3 (1 = 1, 5= 2, 3=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new unique grades:  [1 2 3]\n",
      "                                             comment  grade\n",
      "0  Хотелось долго плеваться после того как я с тр...      1\n",
      "1  ИМХО, в серии о Свароге Бушков исписался. Если...      1\n",
      "2  Много был наслышан об этом авторе и вот взялся...      1\n",
      "3  Да, тяжело читать безграмотную фэнтези. Хотя, ...      1\n",
      "4  Попытка свести сюжет предыдущих книг в единое ...      1\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.DataFrame()\n",
    "comment = []\n",
    "grade = []\n",
    "\n",
    "for grades in unique_grades:\n",
    "    counter = 0\n",
    "    for i in range(len(df)):\n",
    "    #for i in range(000/(len(unique_grades))):\n",
    "        if (counter<300):\n",
    "            if (grades == df['grade'][i]):\n",
    "                comment.append(df['comment'][i])\n",
    "                grade.append(df['grade'][i])\n",
    "                counter = counter + 1 \n",
    "df_new['comment'] = comment\n",
    "#Заменим значения оценок на 1,2,3 \n",
    "#Это облегчит подсчет ошибки \n",
    "#1 - негативный отзыв\n",
    "#2 - нейтральный \n",
    "#3 - положительный\n",
    "df_new['grade'] = grade\n",
    "df_new['grade'] = df_new['grade'].replace(5, 2)\n",
    "df_new['grade'] = df_new['grade'].replace(10, 3)\n",
    "print('new unique grades: ',np.sort(df_new['grade'].unique()))\n",
    "print(df_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('treeclasses_sorted_comments.csv', sep = '|') #save this dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    }
   ],
   "source": [
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_mean_thensors = []\n",
    "grade = []\n",
    "comment = []\n",
    "uncomtiled_rows = 0\n",
    "for i in range(len(df_new)):\n",
    "    try: \n",
    "        array_of_mean_thensors.append(homework4(df_new['comment'][i]))\n",
    "        grade.append(df_new['grade'][i])\n",
    "        comment.append(df_new['comment'][i])\n",
    "    except:\n",
    "        #in some cases, from my point of view, when comment is too large, the data has not processed, and it cause some errors\n",
    "        uncomtiled_rows = uncomtiled_rows + 1\n",
    "        #print('Too long or to short comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  was not processed\n"
     ]
    }
   ],
   "source": [
    "print(uncomtiled_rows, ' was not processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_thensors = pd.DataFrame()\n",
    "df_with_thensors['comment'] = comment\n",
    "df_with_thensors['thensor'] = array_of_mean_thensors\n",
    "df_with_thensors['grade'] = grade\n",
    "df_with_thensors.to_csv('df_with_thensors_900.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array_of_mean_thensors\n",
    "y = df_with_thensors['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as ac\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def metrics(y_test, y_pred, name_of_alg): \n",
    "    mae = np.mean(abs(y_test - y_pred))\n",
    "    #rmse = np.mean(abs(y_test - y_pred))\n",
    "    rmse = (mse(y_test, y_pred))**0.5\n",
    "    print('\\bMetrics and score for ', name_of_alg,':')\n",
    "    print('Accuracy score from rmse:', 100 - np.mean(100 * (rmse / y_test)))\n",
    "    print('Accuracy score from mae:', 100 - np.mean(100 * (mae / y_test)))\n",
    "    print('classification_report: ')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('confusion matrix')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('root mse')    \n",
    "    print(rmse)\n",
    "    print('root mae')    \n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bMetrics and score for  Random forest :\n",
      "Accuracy score from rmse: 46.51462002512986\n",
      "Accuracy score from mae: 67.85185185185179\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.58      0.59        62\n",
      "           2       0.60      0.50      0.55        64\n",
      "           3       0.59      0.72      0.65        54\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.59      0.60      0.59       180\n",
      "weighted avg       0.60      0.59      0.59       180\n",
      "\n",
      "confusion matrix\n",
      "[[36 12 14]\n",
      " [19 32 13]\n",
      " [ 6  9 39]]\n",
      "root mse\n",
      "0.8595864638818418\n",
      "root mae\n",
      "0.5166666666666667\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(metrics(y_test,y_pred, 'Random forest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"result900_of_random_forest.txt\", \"w\") as file:\n",
    "    file.write('There are the result of Random forest on 5000 rows' + '\\n')\n",
    "    for i in range(len(y_test)):\n",
    "        index = y_test.index[i]\n",
    "        file.write('Comment:' + '\\n')\n",
    "        file.write(df_with_thensors['comment'][index] + '\\n')\n",
    "        file.write('y_target: ' + str(y[index]) + '\\n')\n",
    "        file.write('y_predicted: ' + str(y_pred[i]) + '\\n')\n",
    "        file.write('#######################'+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf1 = GradientBoostingClassifier(n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.fit(X_train, y_train)\n",
    "y_pred1 = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bMetrics and score for  GradientBoostingClassifier :\n",
      "Accuracy score from rmse: 43.1991421846494\n",
      "Accuracy score from mae: 65.43209876543213\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.53      0.53        62\n",
      "           2       0.63      0.52      0.57        64\n",
      "           3       0.59      0.72      0.65        54\n",
      "\n",
      "    accuracy                           0.58       180\n",
      "   macro avg       0.59      0.59      0.58       180\n",
      "weighted avg       0.59      0.58      0.58       180\n",
      "\n",
      "confusion matrix\n",
      "[[33 14 15]\n",
      " [19 33 12]\n",
      " [10  5 39]]\n",
      "root mse\n",
      "0.9128709291752769\n",
      "root mae\n",
      "0.5555555555555556\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(metrics(y_test,y_pred1, 'GradientBoostingClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"result900_of_GBC.txt\", \"w\") as file:\n",
    "    file.write('There are the result of Random forest on 5000 rows' + '\\n')\n",
    "    for i in range(len(y_test)):\n",
    "        index = y_test.index[i]\n",
    "        file.write('Comment:' + '\\n')\n",
    "        file.write(df_with_thensors['comment'][index] + '\\n')\n",
    "        file.write('y_target: ' + str(y[index]) + '\\n')\n",
    "        file.write('y_predicted: ' + str(y_pred1[i]) + '\\n')\n",
    "        file.write('#######################'+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bMetrics and score for  DecisionTreeClassifier :\n",
      "Accuracy score from rmse: 43.1991421846494\n",
      "Accuracy score from mae: 65.43209876543213\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.53      0.53        62\n",
      "           2       0.63      0.52      0.57        64\n",
      "           3       0.59      0.72      0.65        54\n",
      "\n",
      "    accuracy                           0.58       180\n",
      "   macro avg       0.59      0.59      0.58       180\n",
      "weighted avg       0.59      0.58      0.58       180\n",
      "\n",
      "confusion matrix\n",
      "[[33 14 15]\n",
      " [19 33 12]\n",
      " [10  5 39]]\n",
      "root mse\n",
      "0.9128709291752769\n",
      "root mae\n",
      "0.5555555555555556\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(metrics(y_test,y_pred1, 'DecisionTreeClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"DecisionTreeClassifier.txt\", \"w\") as file:\n",
    "    file.write('There are the result of Random forest on 5000 rows' + '\\n')\n",
    "    for i in range(len(y_test)):\n",
    "        index = y_test.index[i]\n",
    "        file.write('Comment:' + '\\n')\n",
    "        file.write(df_with_thensors['comment'][index] + '\\n')\n",
    "        file.write('y_target: ' + str(y[index]) + '\\n')\n",
    "        file.write('y_predicted: ' + str(pred2[i]) + '\\n')\n",
    "        file.write('#######################'+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
